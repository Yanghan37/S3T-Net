from functools import partial
from typing import Dict
import torch
import torch.nn as nn
import torch.nn.functional as F
import timm
from collections import OrderedDict
nonlinearity = partial(F.relu, inplace=True)
class MDC(nn.Module):
    def __init__(self, channel):
        super(MDC, self).__init__()
        self.dilate1 = nn.Conv2d(channel, channel, kernel_size=3, dilation=1, padding=1)
        self.dilate2 = nn.Conv2d(channel, channel, kernel_size=3, dilation=3, padding=3)
        self.dilate3 = nn.Conv2d(channel, channel, kernel_size=3, dilation=11, padding=11)
        self.conv1x1 = nn.Conv2d(channel, channel, kernel_size=1, dilation=1, padding=0)

    def forward(self, x):
        dilate1_out = nonlinearity(self.dilate1(x))
        dilate2_out = nonlinearity(self.conv1x1(self.dilate2(x)))
        dilate3_out = nonlinearity(self.conv1x1(self.dilate2(self.dilate1(x))))
        dilate4_out = nonlinearity(self.conv1x1(self.dilate2(self.dilate1(self.dilate3(x)))))
        out = x + dilate1_out + dilate2_out + dilate3_out + dilate4_out
        return out


class RDoubleConv(nn.Sequential):
    def __init__(self, in_channels, out_channels, mid_channels=None):
        super(RDoubleConv, self).__init__()
        if mid_channels is None:
            mid_channels = out_channels
        self.conv_m = SingleConv(in_channels, out_channels)
        self.conv1 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

    def forward(self, x):
        x = self.conv_m(x)
        identity = x
        out = self.bn2(self.conv2(self.relu(self.bn1(self.conv1(x)))))
        out += identity
        out = self.relu(out)
        return out
class SingleConv(nn.Sequential):
    def __init__(self, in_channels, out_channels, mid_channels=None):
        if mid_channels is None:
            mid_channels = out_channels
        super(SingleConv, self).__init__(
            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(mid_channels),
            nn.ReLU(inplace=True),
        )

class Up(nn.Module):
    def __init__(self, in_channels, out_channels, bilinear=True):
        super(Up, self).__init__()
        
        class DoubleConv(nn.Sequential):
            def __init__(self, in_channels, out_channels, mid_channels=None):
                if mid_channels is None:
                    mid_channels = out_channels
                super(DoubleConv, self).__init__(
                    nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),
                    nn.BatchNorm2d(mid_channels),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),
                    nn.BatchNorm2d(out_channels),
                    nn.ReLU(inplace=True)
                )
        
        if bilinear:
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)
        else:
            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)
            self.conv = DoubleConv(in_channels, out_channels)

    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:
        x1 = self.up(x1)
        
        diff_y = x2.size()[2] - x1.size()[2]
        diff_x = x2.size()[3] - x1.size()[3]
        
        x1 = F.pad(x1, [diff_x // 2, diff_x - diff_x // 2,
                        diff_y // 2, diff_y - diff_y // 2])

        x = torch.cat([x2, x1], dim=1)
        x = self.conv(x)
        return x

class OutConv(nn.Sequential):
    def __init__(self, in_channels, num_classes):
        super(OutConv, self).__init__(
            nn.Conv2d(in_channels, num_classes, kernel_size=1)
        )
def conv(in_channels, out_channels, kernel_size, bias=False, stride=1):
    return nn.Conv2d(
        in_channels, out_channels, kernel_size,
        padding=(kernel_size // 2), bias=bias, stride=stride)

class DimensionMatchingLayer(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(DimensionMatchingLayer, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)
    def forward(self, x):
        x = self.conv(x)
        return x 

class SynergisticWeightedFeatureConfluence(nn.Module):
    def __init__(self, feature_channels, output_dim, r=4):
        super(SynergisticWeightedFeatureConfluence, self).__init__()
        self.feature_channels = feature_channels
        self.output_dim = output_dim
        self.conv3x3 = nn.Conv2d(feature_channels * 2, feature_channels, kernel_size=3, padding=1)
        self.conv1x1 = nn.Conv2d(feature_channels, output_dim, kernel_size=1)
        inter_channels = int(feature_channels // r)
        self.local_att = nn.Sequential(
            nn.Conv2d(feature_channels, inter_channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(inter_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(inter_channels, feature_channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(feature_channels),
        )
        self.global_att = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(feature_channels, inter_channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(inter_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(inter_channels, feature_channels, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(feature_channels),
        )
        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)
        self.sigmoid = nn.Sigmoid()
    def forward(self, e1, y1):
        concat_features = torch.cat((e1, y1), dim=1)
        g = self.conv3x3(concat_features)
        g = self.conv1x1(g)
        xl = self.local_att(g)
        xg = self.global_att(g)
        xlg = xl + xg
        wei = self.sigmoid(xlg)
        t_weighted = e1 * wei
        f_weighted = y1 * wei
        z_af = torch.cat((t_weighted,f_weighted),dim=1)
        A = self.sigmoid(z_af)
        A1, A2 = torch.split(A, self.feature_channels, dim=1)
        e1 = e1 * A1
        y1 = y1 * A2
        z_af = e1 + y1
        return z_af

class SKAttention(nn.Module):
    def __init__(self, channel, kernels=[1, 3, 5, 7], reduction=16, group=1, L=32):
        super().__init__()
        self.d = max(L, channel // reduction)
        self.convs = nn.ModuleList([])
        for k in kernels:
            self.convs.append(nn.Sequential(OrderedDict([
                ('conv', nn.Conv2d(channel, channel, kernel_size=k, padding=k//2, groups=group)),
                ('bn', nn.BatchNorm2d(channel)),
                ('relu', nn.ReLU())
            ])))
        self.fc = nn.Linear(channel, self.d)
        self.fcs = nn.ModuleList([])
        for i in range(len(kernels)):
            self.fcs.append(nn.Linear(self.d, channel))
        self.softmax = nn.Softmax(dim=0)
    def forward(self, x):
        bs, c, _, _ = x.size()
        conv_outs = []
        for conv in self.convs:
            conv_outs.append(conv(x))
        feats = torch.stack(conv_outs, 0)
        U = sum(conv_outs)
        S = U.mean(-1).mean(-1)
        Z = self.fc(S)
        weights = []
        for fc in self.fcs:
            weight = fc(Z)
            weights.append(weight.view(bs, c, 1, 1))
        attention_weights = torch.stack(weights, 0)
        attention_weights = self.softmax(attention_weights)
        V = (attention_weights * feats).sum(0)
        return V
class SpectralHierarchicalDualDomainAttention(nn.Module):
    def __init__(self, dim, heads=4):
        super().__init__()
        self.heads = heads
        self.dwconv = nn.Conv2d(dim // 2, dim // 2, 3, padding=1, groups=dim // 2)
        self.qkvl = nn.Conv2d(dim // 2, (dim // 4) * heads, 1)
        self.pool_q = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)
        self.pool_k = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        self.act = nn.GELU()
        self.q1 = nn.Conv2d(dim, dim // 4, kernel_size=1)
        self.k1 = nn.Conv2d(dim, dim // 4, kernel_size=1)
        self.mlp = nn.Sequential(
            nn.Linear(dim // 2, dim),
            nn.GELU(),
            nn.Linear(dim, dim // 2)
        )
    def high_pass_filter(self, x_fft, threshold=10):
        return x_fft * (torch.abs(x_fft) > threshold)
    def multi_scale_fft(self, x, scales=[1, 2, 4]):
        fft_results = []
        for scale in scales:
            x_scaled = F.avg_pool2d(x, scale) if scale > 1 else x
            fft_results.append(self.high_pass_filter(torch.fft.fftn(x_scaled, dim=(-2, -1))))
        return sum(self.interpolate_fft(fft, x.shape[-2:]) for fft in fft_results)
    def interpolate_fft(self, x_fft, target_size):
        return torch.complex(
            F.interpolate(x_fft.real, size=target_size, mode='bilinear', align_corners=False),
            F.interpolate(x_fft.imag, size=target_size, mode='bilinear', align_corners=False)
        )
    def forward(self, x):
        B, C, H, W = x.shape
        x1, x2 = torch.split(x, [C // 2, C // 2], dim=1)
        x1 = self.act(self.dwconv(x1))
        x2 = self.act(self.qkvl(x2))
        x_fft = torch.fft.fftshift(self.multi_scale_fft(x2))
        q1, k1 = self.q1(x_fft.real), self.k1(x_fft.imag)
        x2 = x2.reshape(B, self.heads, C // 4, H, W)
        q, k, v, lfeat = torch.sum(x2[:, :-3], dim=1), x2[:, -3], x2[:, -2].flatten(2), x2[:, -1]
        attention = torch.softmax(torch.matmul(q.flatten(2), k.flatten(2).transpose(1, 2)), dim=1)
        attention = torch.softmax(torch.matmul(q1.flatten(2), k1.flatten(2).transpose(1, 2)), dim=1)+attention
        attention_result = torch.matmul(attention, v).reshape(B, C // 4, H, W)
        x1_mlp = self.mlp(x1.flatten(2).transpose(1, 2)).transpose(1, 2).reshape(B, C // 2, H, W)
        return torch.cat([x1_mlp, lfeat, attention_result], dim=1)
class  RecursiveFrequencySpaceRefinementModule(nn.Module):
    def __init__(self, in_channels, num_iterations=3, reduction=32, groups=2):
        super(RecursiveFrequencySpaceRefinementModule, self).__init__()
        self.groups = groups
        self.num_iterations = num_iterations
        self.sk_attention = SKAttention(channel=in_channels//2, kernels=[1, 3], reduction=8, L=16)
        self.sigmoid = nn.Sigmoid()
        self.sdc = MDC(in_channels//2)
        self.singleconv = SingleConv(in_channels//2, in_channels//2)
        self.space_conv = nn.Conv2d(in_channels//2, in_channels//2, kernel_size=1) 

    def channel_shuffle(self, x, groups):
        B, C, H, W = x.shape
        return x.view(B, groups, C // groups, H, W).permute(0, 2, 1, 3, 4).contiguous().view(B, -1, H, W)

    def forward(self, x):
        x = self.channel_shuffle(x, self.groups)
        space_path_in, fft_path = torch.chunk(x, 2, dim=1)
        fft_x = torch.fft.fftn(fft_path, dim=(-2, -1), norm='ortho')
        fft_x_real, fft_x_imag = fft_x.real, fft_x.imag
        space_path_in = self.sk_attention(self.space_conv(space_path_in))
        fft_x_real = self.sk_attention(fft_x_real)
        
        for _ in range(self.num_iterations):
            attn = self.sigmoid(self.singleconv(self.sdc(fft_x_real)) + self.singleconv(self.sdc(space_path_in)))
            fft_x_real = fft_x_real * attn
            space_path_in = space_path_in * attn
        ifft_x = torch.fft.irfftn(torch.complex(fft_x_real, fft_x_imag), s=x.shape[-2:], dim=(-2, -1), norm='ortho')
        return torch.cat((ifft_x, space_path_in), dim=1)


from pytorch_wavelets import DWTForward
class Down_wt(nn.Module):    
    def __init__(self, in_ch, out_ch):
        super(Down_wt, self).__init__()
        self.wt = DWTForward(J=1, mode='zero', wave='haar')  
        self.conv_bn_relu = nn.Sequential(
            nn.Conv2d(in_ch * 4, out_ch, kernel_size=1, stride=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )

    def forward(self, x):
        yL, yH = self.wt(x) 
        y_HL = yH[0][:, :, 0, ::]
        y_LH = yH[0][:, :, 1, ::]
        y_HH = yH[0][:, :, 2, ::]
        x = torch.cat([yL, y_HL, y_LH, y_HH], dim=1)
        x = self.conv_bn_relu(x)
        return x
    

class S3T-Net(nn.Module):
    def __init__(self,
                 in_channels: int = 1,
                 num_classes: int = 2,
                 bilinear: bool = True,
                 base_c: int = 64):
        super(S3T-Net, self).__init__()
        self.in_channels = in_channels
        self.num_classes = num_classes
        self.bilinear = bilinear
        self.in_conv = RDoubleConv(in_channels, base_c)
        self.down_wt_0 = Down_wt(base_c, base_c)
        self.down_wt_1 = Down_wt(base_c, base_c * 2)
        self.down_wt_2 = Down_wt(base_c * 2, base_c * 4)
        self.down_wt_3 = Down_wt(base_c * 4, base_c * 8)
        factor = 2 if bilinear else 1
        self.down_wt_4 = Down_wt(base_c * 8, base_c * 16 // factor)
        self.up1 = Up(base_c * 16, base_c * 8 // factor, bilinear)
        self.up2 = Up(base_c * 8, base_c * 4 // factor, bilinear)
        self.up3 = Up(base_c * 4, base_c * 2 // factor, bilinear)
        self.up4 = Up(base_c * 2, base_c, bilinear)
        self.out_conv = OutConv(base_c, num_classes)
        self.dim_match_layer1= DimensionMatchingLayer(96, 64)
        self.dim_match_layer2 = DimensionMatchingLayer(96, 128)
        self.dim_match_layer3= DimensionMatchingLayer(192, 256)
        self.dim_match_layer4= DimensionMatchingLayer(384, 512)
        self.dim_match_layer5= DimensionMatchingLayer(768, 1024)  

        self.vit = timm.create_model(
          'maxxvit_rmlp_small_rw_256.sw_in1k',pretrained=True,                                 
            features_only=True,)  
        self.SWFC64=SynergisticWeightedFeatureConfluence(64,64)
        self.SWFC128=SynergisticWeightedFeatureConfluence(128,128)
        self.SWFC256=SynergisticWeightedFeatureConfluence(256,256)
        self.SWFC512=SynergisticWeightedFeatureConfluence(512,512)
        self.SWFC1024=SynergisticWeightedFeatureConfluence(1024,1024)

        self.RFSR= RecursiveFrequencySpaceRefinementModule(64)
        self.RFSR128 = RecursiveFrequencySpaceRefinementModule(128)
        self.RFSR256 = RecursiveFrequencySpaceRefinementModule(256)
        self.RFSR512 = RecursiveFrequencySpaceRefinementModule(512)
        self.RFSR1024 = RecursiveFrequencySpaceRefinementModule(1024)

        self.SHDA64 = SpectralHierarchicalDualDomainAttention(dim=64, heads=4)
        self.SHDA128 = SpectralHierarchicalDualDomainAttention(dim=128, heads=4)
        self.SHDA256 = SpectralHierarchicalDualDomainAttention(dim=256, heads=4)
        self.SHDA512 = SpectralHierarchicalDualDomainAttention(dim=512, heads=4)
        self.SHDA1024 = SpectralHierarchicalDualDomainAttention(dim=1024, heads=4)
    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        vit_features = self.vit(x)
        v1, v2, v3, v4, v5 = vit_features[:5] 
        v1=self.dim_match_layer1(v1)
        v2=self.dim_match_layer2(v2)
        v3=self.dim_match_layer3(v3)
        v4=self.dim_match_layer4(v4)
        v5=self.dim_match_layer5(v5)
        e0 = self.in_conv(x) 
        e1 = self.down_wt_0(e0)
        e1=self.SHDA64(e1)
        e2 = self.down_wt_1(e1)
        e2=self.SHDA128(e2)        
        e3 = self.down_wt_2(e2)
        e3=self.SHDA256(e3)    
        e4 = self.down_wt_3(e3)
        e4=self.SHDA512(e4)        
        e5= self.down_wt_4(e4)
        e5=self.SHDA1024(e5)
        x1=self.SWFC64(e1,v1)
        x2=self.SWFC128(e2,v2)
        x3=self.SWFC256(e3,v3)
        x4=self.SWFC512(e4,v4)
        x5=self.SWFC1024(e5,v5)
        x = self.up1(x5, x4)
        x=self.RFSR512(x)
        x = self.up2(x, x3)
        x=self.RFSR256(x)
        x = self.up3(x, x2)
        x=self.RFSR128(x)
        x = self.up4(x, x1)
        x=self.RFSR(x)
        x=F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)
        logits = self.out_conv(x)
        return logits
    
